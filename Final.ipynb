{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Parameters Influencing Human Strikezones\n",
    "\n",
    "## Goals\n",
    "\n",
    "The objective of this project is to determine what factors other than position influence an umpire's accuracy when calling balls and strikes. I plan to look at factors such as inning and batter's handedness that should not influence the data but possibly does as well as factors such as batter's height that should influence ball/strike calls.\n",
    "\n",
    "## Methods\n",
    "\n",
    "I have created multiple models based on a pitch's x and y coordinates as they cross the plate in order to set up a baseline for accuracy. I then will predict data using this model and find the subsets that are more or less accurate. Finally, I will add the factors that most directly influence the data as explanatory variables for a final model.\n",
    "\n",
    "## The data\n",
    "\n",
    "The pitch data was collected and published by major league baseball. It has been collected into nice csv files on kaggle (https://www.kaggle.com/pschale/mlb-pitch-data-20152018). Pitches included are from the 2015-2018 seasons. Additionally this dataset includes seperate csvs containing data containing information about the atbat (ie inning, batter, pitcher) and game (ie umpire, weather). I have joined the data from the pitch csv with other data to \n",
    "\n",
    "## Results\n",
    "\n",
    "While the initial models predicted balls and strikes with a solid accuracy, I was unable to find additional parameters that significantly improved performance. Batter handedness, inning, and home/away batter had very little difference in model accuracy, and adding them to the models improved performance at best marginally.\n",
    "\n",
    "Umpires, however, do have varying levels of accuracy. I was able to use the model to find umpires that were better or worse at calling strikes. However, even upon creating a model that added the umpire as an explantory variabele, the accuracy increased only marginally.\n",
    "\n",
    "Overall, it appears that umpires are, in the end, pretty good at their jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data\n",
    "\n",
    "This block imports the data from the csv files. It also does some transformations on the most important columns, converting the \"Code\" column to \"Call\", which is true for strikes and false for balls. It also removes all outcomes that are not balls or called strikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tenor\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "c:\\users\\tenor\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    }
   ],
   "source": [
    "#Data importing and manipulating, mostly from step 1\n",
    "#We'll also import everything needed for every block here so each model can be ran without relying on any prior blocks.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os.path\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "games = pd.read_csv(\"games.csv\")\n",
    "atbats= pd.read_csv(\"atbats.csv\")\n",
    "players = pd.read_csv(\"player_names.csv\")\n",
    "pitches_full = pd.read_csv(\"pitches.csv\")\n",
    "pitches = pitches_full\n",
    "pitch = pitches[['px', 'pz', 'code']][pitches['code'].isin([\"C\", \"B\", \"*B\"])] #only balls and called strikes\n",
    "\n",
    "\n",
    "#These two rows are most important for step 2, as they clean the data and transform one column.\n",
    "pitch = pitches[['px', 'pz', 'code', 'ab_id']][pitches['code'].isin([\"C\", \"B\"])].dropna(axis='rows', how='any') #drop NA's\n",
    "pitch['call'] = pitches['code'].isin([\"C\"]) #Call will be True for strikes, False for balls\n",
    "\n",
    "\n",
    "strikes = pitch[['px', 'pz']][pitches['code'].isin([\"C\"])] #We use these later for graphical summaries\n",
    "balls = pitch[['px', 'pz']][pitches['code'].isin([\"B\"])]\n",
    "\n",
    "#Pickle is a great package that stores python variables as files. This is really useful as it saves me from running\n",
    "#every model every time I want to run this document. It also allows me to share the models online so others can view the results immediately.\n",
    "#The .pkl files can be found at the link below.\n",
    "#https://github.com/JacksonTenor/Influences-on-Umpire-Accuracy\n",
    "use_pickle = True\n",
    "#When using pickle the train/test split is pointless, as pickle doesn't store the test data\n",
    "#Therefore, it is testing on data it was trained on. When I initially ran this, that didn't happen\n",
    "#And the results were still around the same, so it still works for demonstration.\n",
    "\n",
    "#This function is used to get the accuracy of predictions later in the document.\n",
    "def getAccuracy(predictions, actual):\n",
    "    return (np.sum(predictions == actual) / (len(actual)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Models\n",
    "\n",
    "Using only the positional data as the ball crosses the plate, I created four models to predict whether the pitch would be called a ball or a strike. The first, a logistic regression, had an underwhelming performance. The second, a nearest neighbors model, performed quite well. This performance was matched by both of the nerual networks I trained. \n",
    "\n",
    "After evaluating these four models, I took the confidences of one neural network and the nearest neighbors model and averaged them to create a final model, although this step similarly did not improve accuracy.\n",
    "\n",
    "Given that many of these models have topped out at around a 91.5% accuracy, I infer that given these explanatory paramters, this level of accuracy is near the limit we can get due to variance in umpire calls.\n",
    "\n",
    "Each of these models is tested on a retained portion of the data set equal to 20% of the total data to avoid overfitting and ensure accurate results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "### Process:\n",
    "I began with a simple logistic regression. It was a simple, although relatively unsuccessful starting point. I first tried giving it px and pz values, but the model just predicted everything to be a ball. However, when giving it distance from the center of the observed values it was able to find success.\n",
    "\n",
    "### Results:\n",
    "In the end I was able to create a moderately successful classifier with 88.7% accuracy by using the distance from center as an explanatory variable.\n",
    "\n",
    "### Remaining Questions:\n",
    "There is no guarentee the mean of the px and pz values is the best center point for this regression. It would be possible to try measuring distance from a variety of points and seeing which would perform the best, but I belive other models will outperform even the most optimized logistic regression for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tenor\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6669328432893852\n"
     ]
    }
   ],
   "source": [
    "#My first attempt at a regression, which found no success\n",
    "in_full = pitch[['px', 'pz']].values\n",
    "out_full = pitch['call']\n",
    "\n",
    "in_train, in_test, out_train, out_test = train_test_split(in_full, out_full, test_size = 0.20)\n",
    "\n",
    "#Now we train the model\n",
    "model = lm.LogisticRegression()\n",
    "model.fit(in_train, out_train)\n",
    "\n",
    "score = model.score(in_test, out_test)\n",
    "print(\"Score: \" + str(score))\n",
    "#That's just the proportion of balls. Not good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.8869679228111041\n",
      "88.7% accuracy is not terrible!\n"
     ]
    }
   ],
   "source": [
    "#This regression found decent success!\n",
    "\n",
    "#I use distance from the center of our observations as our input metric.\n",
    "#Distances have had greater success than pure px and pz values\n",
    "#And by measuring distance from the mean we should get distance from roughly the center of the strike zone. \n",
    "mean_x = np.mean(pitch['px'])\n",
    "mean_z = np.mean(pitch['pz'])\n",
    "pitch['x_dist'] = pitch['px'] - mean_x\n",
    "pitch['z_dist'] = pitch['pz'] - mean_z\n",
    "pitch['distance'] = np.sqrt(pitch['x_dist'] * pitch['x_dist'] + pitch['z_dist'] * pitch['z_dist'])\n",
    "#We split our data into training and test sets\n",
    "\n",
    "in_full = pitch[['distance']].values\n",
    "out_full = pitch['call']\n",
    "\n",
    "in_train, in_test, out_train, out_test = train_test_split(in_full, out_full, test_size = 0.20)\n",
    "\n",
    "\n",
    "if not use_pickle or not os.path.isfile('lm.pkl'):\n",
    "    #Now we train the model\n",
    "    linear_model = lm.LogisticRegression()\n",
    "    linear_model.fit(in_train, out_train)\n",
    "    if use_pickle:\n",
    "        f=open('lm.pkl', 'wb')\n",
    "        pickle.dump(linear_model, f)\n",
    "        f.close()\n",
    "else:\n",
    "    f=open('lm.pkl', 'rb')\n",
    "    linear_model = pickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "    \n",
    "score = linear_model.score(in_test, out_test)\n",
    "print(\"Score: \" + str(score))\n",
    "print(\"88.7% accuracy is not terrible!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbors\n",
    "\n",
    "### Process:\n",
    "As an attempt to create a better model than the logistic regression, I decided to use a K nearest neighbors model. Immediately I was getting success above 90%, but to maximize the model's accuracy I looped through different values for n_neighbors to find the optimal one, which ended up being around 95. I also tried weighting based on distance, which had minimal impact on the model. \n",
    "\n",
    "### Results:\n",
    "The end models were extremely successful with 91.5% for the unweighted model and 91.2% for the distance weighted model.\n",
    "This is quite good!\n",
    "\n",
    "### Remaining Questions:\n",
    "I believe if I spent as much time tweaking the distance weighted model as I did the non-weighted model it could perform as well or better. I also would be interested if there are any meaningful differences in the confidences of these two models\n",
    "The first model is a simple fraction over 95, but I would believe this one could be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here's where we will do the much more successful K-nearest neighbors model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We split our data into training and test sets again\n",
    "in_full = pitch[['px', 'pz']].values\n",
    "out_full = pitch['call']\n",
    "\n",
    "in_train, in_test, out_train, out_test = train_test_split(in_full, out_full, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I reccomend not running the following code block due to runtime. It creates 50  models with varying n_neigbors values to find the optimal one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lest's find the optimal number of neighbors\n",
    "#In the end I had the best results with n=95\n",
    "#This code block takes a long time to run, I would reccomend ignoring it and just running the next one with the actual model.\n",
    "if(False):\n",
    "    knn_list_50s = []\n",
    "    def KNN(n):\n",
    "        classifier = KNeighborsClassifier(n_neighbors = n)\n",
    "        classifier.fit(in_train, out_train)\n",
    "\n",
    "        #Predicting, this takes a while\n",
    "        test_predictions = classifier.predict(in_test)\n",
    "        return np.sum(test_predictions == out_test) / len(test_predictions)\n",
    "\n",
    "    for i in range(1, 50):\n",
    "        knn_list_50s.append(KNN(i * 5))\n",
    "    knn_list_50s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9140377366385728\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 95)\n",
    "knn.fit(in_train, out_train)\n",
    "\n",
    "#Predicting, this takes a while\n",
    "test_predictions = knn.predict(in_test)\n",
    "    \n",
    "print(str(np.sum(test_predictions == out_test) / len(test_predictions)))\n",
    "#91.5% Accuracy, pretty good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9114991788044369\n"
     ]
    }
   ],
   "source": [
    "knn_distance = KNeighborsClassifier(n_neighbors = 95, weights=\"distance\")\n",
    "knn_distance.fit(in_train, out_train)\n",
    "\n",
    "#Predicting, this takes a while\n",
    "test_predictions = knn_distance.predict(in_test)\n",
    "#Predicting probabilities, I should do this at the same time as above, but I don't because I'm dumb.\n",
    "probabilities = knn_distance.predict_proba(in_test)\n",
    "print(str(np.sum(test_predictions == out_test) / len(test_predictions)))\n",
    "#91.2% Accuracy \n",
    "#Not bad. I believe if I spent as much time tweaking this as I did the non-weighted model it could perform as well or better.\n",
    "#I also would be interested if there are any meaningful differences in the confidences of these two models\n",
    "#The first model is a simple fraction over 95, but I would believe this one could be different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "### Process:\n",
    "For my neural nets I used relu as the activation function. The documentation for this package reccomended using the 'adam' solver for larger datasets, so I utilized that. I trained a net with two hidden layers with 64 neurons each as well as a larger one with three hidden layers of size 128.\n",
    "\n",
    "### Results:\n",
    "Both of these models peformed similary to eachother and to the KNN model. This leads me to believe that, given this data set and these parameters, the best we may be able to do is around 91.5%.\n",
    "\n",
    "As one would expect, these take a while to train, so use of the pickle package is highly reccomended here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9147072463970263\n"
     ]
    }
   ],
   "source": [
    "#Neural Network time!\n",
    "mean_x = np.mean(pitch['px'])\n",
    "mean_z = np.mean(pitch['pz'])\n",
    "#Interestingly, although in the end relatively unimportant, the standard deviation of this data ends up being around 1\n",
    "#Therefore the z scores used here and the distances in the regression example are extremly similar\n",
    "pitch['z_px'] = (pitch['px'] - mean_x)/pitch.px.std(ddof=0) \n",
    "pitch['z_pz'] = (pitch['pz'] - mean_z)/pitch.pz.std(ddof=0)\n",
    "\n",
    "#We split our data into training and test sets again (again)\n",
    "in_full = pitch[['z_px', 'z_pz']].values\n",
    "out_full = pitch['call']\n",
    "\n",
    "in_train, in_test, out_train, out_test = train_test_split(in_full, out_full, test_size = 0.20)\n",
    "\n",
    "if not use_pickle or not os.path.isfile('mlp_small.pkl'):\n",
    "\n",
    "    #adam is reccomended for datasets with samples over 1000s\n",
    "    #early stopping will set aside a validation set and stop when that is no longer improving\n",
    "    mlp = MLPClassifier(solver='adam', activation='relu', hidden_layer_sizes=(64, 64), random_state=1, early_stopping = True)\n",
    "\n",
    "    mlp.fit(in_train, out_train)\n",
    "    if use_pickle:\n",
    "        f=open('mlp_small.pkl', 'wb')\n",
    "        pickle.dump(mlp,f)\n",
    "        f.close()\n",
    "else:\n",
    "    f=open('mlp_small.pkl', 'rb')\n",
    "    mlp = pickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "\n",
    "print(getAccuracy(mlp.predict(in_test), out_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9146340187671954\n"
     ]
    }
   ],
   "source": [
    "#Neural net 2, more hidden layers this time because why not?\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mean_x = np.mean(pitch['px'])\n",
    "mean_z = np.mean(pitch['pz'])\n",
    "#Interestingly, although in the end relatively unimportant, the standard deviation of this data ends up being around 1\n",
    "#Therefore the z scores used here and the distances in the regression example are extremly similar\n",
    "pitch['z_px'] = (pitch['px'] - mean_x)/pitch.px.std(ddof=0) \n",
    "pitch['z_pz'] = (pitch['pz'] - mean_z)/pitch.pz.std(ddof=0)\n",
    "\n",
    "#We split our data into training and test sets again (again)\n",
    "in_full = pitch[['z_px', 'z_pz']].values\n",
    "out_full = pitch['call']\n",
    "\n",
    "in_train, in_test, out_train, out_test = train_test_split(in_full, out_full, test_size = 0.20)\n",
    "\n",
    "if not use_pickle or not os.path.isfile('mlp_large.pkl'):\n",
    "    #adam is reccomended for datasets with samples over 1000s\n",
    "    #early stopping will set aside a validation set and stop when that is no longer improving\n",
    "    mlp_large = MLPClassifier(solver='adam', activation='relu', hidden_layer_sizes=(128, 128, 128), random_state=1, early_stopping = True)\n",
    "\n",
    "    mlp_large.fit(in_train, out_train)\n",
    "    if use_pickle:\n",
    "        f=open('mlp_large.pkl', 'wb')\n",
    "        pickle.dump(mlp_large, f)\n",
    "        f.close()\n",
    "else:\n",
    "    f=open('mlp_large.pkl', 'rb')\n",
    "    mlp_large = pickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "\n",
    "print(getAccuracy(mlp_large.predict(in_test), out_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_x = np.mean(pitch['px'])\n",
    "mean_z = np.mean(pitch['pz'])\n",
    "#Interestingly, although in the end relatively unimportant, the standard deviation of this data ends up being around 1\n",
    "#Therefore the z scores used here and the distances in the regression example are extremly similar\n",
    "pitch['z_px'] = (pitch['px'] - mean_x)/pitch.px.std(ddof=0) \n",
    "pitch['z_pz'] = (pitch['pz'] - mean_z)/pitch.pz.std(ddof=0)\n",
    "\n",
    "#The models probabilites are returned as a 2d list where each element has the probability of it being \n",
    "    \n",
    "\n",
    "knn_prob = knn.predict_proba(pitch[['px', 'pz']])\n",
    "knn_ps = []\n",
    "for i in knn_prob:\n",
    "    knn_ps.append(i[1])\n",
    "    \n",
    "pitch['knn_prob'] = knn_ps\n",
    "\n",
    "\n",
    "nn_prob = mlp.predict_proba(pitch[['z_px', 'z_pz']])\n",
    "nn_ps = []\n",
    "for i in nn_prob:\n",
    "    nn_ps.append(i[1])\n",
    "\n",
    "pitch['nn_prob'] = nn_ps\n",
    "\n",
    "\n",
    "pitch['prob'] = (pitch['knn_prob'] + pitch['nn_prob']) / 2\n",
    "pitch['predict'] = pitch['prob'] > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9149358769160385% accuracy for the combined model\n",
      "We don't really gain any accuracy combining these models, but I'm going to be using it from here on out.\n"
     ]
    }
   ],
   "source": [
    "print(str(np.sum(pitch['predict'] == pitch['call']) / len(pitch)) + \" accuracy for the combined model\")\n",
    "print(\"We don't really gain any accuracy combining these models, but I'm going to be using it from here on out.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Location parameters\n",
    "\n",
    "To use non-location based parameters I had to join the csv files together. Fortunately the csv files are formated as if they were exported straight from a SQL database, as the id fields work perfectly as primary/foreign keys for joining on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = atbats[['ab_id', 'batter_id', 'p_throws', 'stand', 'pitcher_id', 'top', 'inning']]\n",
    "\n",
    "#This is where it would have been nice to import the data into a SQL database\n",
    "#But the benefit of not doing that is that this is much easier to share. My last project\n",
    "#Was difficult to share online due to the database hosting\n",
    "\n",
    "#Joining the atbats and pitches on ab_id\n",
    "p_ab = pd.merge(ab, pitch, on='ab_id')\n",
    "\n",
    "#Joining pitch/atbat dataframe with the game csv on g_id (game id)\n",
    "p_ab = pd.merge(p_ab, atbats[['ab_id', 'g_id']], on='ab_id')\n",
    "p_ab = pd.merge(games[['umpire_HP', 'g_id']], p_ab, on='g_id')\n",
    "\n",
    "\n",
    "p_ab['correct'] = p_ab['predict'] == p_ab['call']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batter Handedness\n",
    "\n",
    "I predicted batter handedness to be one of the most influential factors. However, to my surprise, the accuracy of our model was not significantly different for lefties or righties and adding handedness to the nearest neighbors model only improved it marginally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on left handed batters: 0.9147242500098295\n",
      "Accuracy on right handed batters: 0.9150927471993685\n"
     ]
    }
   ],
   "source": [
    "#Splitting on left and right\n",
    "left = p_ab.loc[p_ab['stand'] == \"L\"]\n",
    "right = p_ab.loc[p_ab['stand'] == \"R\"]\n",
    "\n",
    "print(\"Accuracy on left handed batters: \" + str(getAccuracy(left['predict'], left['call'])))\n",
    "print(\"Accuracy on right handed batters: \" + str(getAccuracy(right['predict'], right['call'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-810008927b9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mknn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_neighbors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m95\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mknn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m#Predicting, this takes a while\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tenor\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\neighbors\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    914\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 916\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    917\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tenor\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\neighbors\\base.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    256\u001b[0m             self._tree = KDTree(X, self.leaf_size,\n\u001b[0;32m    257\u001b[0m                                 \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meffective_metric_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m                                 **self.effective_metric_params_)\n\u001b[0m\u001b[0;32m    259\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_method\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'brute'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stand_float = []\n",
    "for i in p_ab['stand']:\n",
    "    if i == \"L\":\n",
    "        stand_float.append(5) #Semi-Arbitrary distance\n",
    "    else:\n",
    "        stand_float.append(0)\n",
    "p_ab['stand_float'] = stand_float\n",
    "in_full = p_ab[['px', 'pz', 'stand_float']].values\n",
    "out_full = p_ab['call']\n",
    "\n",
    "in_train, in_test, out_train, out_test = train_test_split(in_full, out_full, test_size = 0.20)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 95)\n",
    "knn.fit(in_train, out_train)\n",
    "\n",
    "#Predicting, this takes a while\n",
    "test_predictions = knn.predict(in_test)\n",
    "    \n",
    "print(str(np.sum(test_predictions == out_test) / len(test_predictions)))\n",
    "print(\"Considering our 91.45% accuracy for our base k-nearest neigbors model, a 91.73% accuracy isn't a big improvement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Home/Away\n",
    "\n",
    "While I don't expect there to be a significant difference in calls for the home or away team, finding a difference in this area would be problematic, so I decided to check. Fortuantely, based both on the lack of difference in the model's accuracy on home team at bats vs away team at bats as well as the model's lack of improvement when considering the half of the inning indicates that this is not something to worry about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model accuracy on away team at bats: 0.9145048035557702%\n",
      "Original model accuracy on home team at bats: 0.9153813645852242%\n"
     ]
    }
   ],
   "source": [
    "away_atbats = p_ab.loc[p_ab[\"top\"] == True]\n",
    "home_atbats = p_ab.loc[p_ab[\"top\"] == False]\n",
    "\n",
    "print(\"Original model accuracy on away team at bats: \" + str(getAccuracy(away_atbats['predict'], away_atbats['call'])) + \"%\")\n",
    "print(\"Original model accuracy on home team at bats: \" + str(getAccuracy(home_atbats['predict'], home_atbats['call'])) + \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_float = []\n",
    "for i in p_ab['top']:\n",
    "    if i:\n",
    "        top_float.append(1)\n",
    "    else:\n",
    "        top_float.append(0)\n",
    "p_ab['top_float'] = top_float\n",
    "in_full = p_ab[['px', 'pz', 'top_float']].values\n",
    "out_full = p_ab['call']\n",
    "\n",
    "in_train, in_test, out_train, out_test = train_test_split(in_full, out_full, test_size = 0.20)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 95)\n",
    "knn.fit(in_train, out_train)\n",
    "\n",
    "#Predicting, this takes a while\n",
    "test_predictions = knn.predict(in_test)\n",
    "print(str(np.sum(test_predictions == out_test) / len(test_predictions)))\n",
    "print(\"The 0.01% improvement isn't exactyly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inning Number\n",
    "\n",
    "atbats.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "in_full = p_ab[['px', 'pz', 'inning']].values\n",
    "out_full = p_ab['call']\n",
    "\n",
    "in_train, in_test, out_train, out_test = train_test_split(in_full, out_full, test_size = 0.20)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 95)\n",
    "knn.fit(in_train, out_train)\n",
    "\n",
    "#Predicting, this takes a while\n",
    "test_predictions = knn.predict(in_test)\n",
    "print(str(np.sum(test_predictions == out_test) / len(test_predictions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "if not use_pickle or not os.path.isfile('mlp_inning_2.pkl'):\n",
    "    mean_x = np.mean(p_ab['px'])\n",
    "    mean_z = np.mean(p_ab['pz'])\n",
    "    #Interestingly, although in the end relatively unimportant, the standard deviation of this data ends up being around 1\n",
    "    #Therefore the z scores used here and the distances in the regression example are extremly similar\n",
    "    p_ab['z_px'] = (p_ab['px'] - mean_x)/p_ab.px.std(ddof=0) \n",
    "    p_ab['z_pz'] = (p_ab['pz'] - mean_z)/p_ab.pz.std(ddof=0)\n",
    "\n",
    "    #We split our data into training and test sets again (again)\n",
    "    in_full = p_ab[['z_px', 'z_pz', 'inning']].values\n",
    "    out_full = p_ab['call']\n",
    "\n",
    "    in_train, in_test, out_train, out_test = train_test_split(in_full, out_full, test_size = 0.20)\n",
    "\n",
    "    #adam is reccomended for datasets with samples over 1000s\n",
    "    #early stopping will set aside a validation set and stop when that is no longer improving\n",
    "    mlp_inning = MLPClassifier(solver='adam', activation='relu', hidden_layer_sizes=(64, 64), random_state=1, early_stopping = True)\n",
    "\n",
    "    mlp_inning.fit(in_train, out_train)\n",
    "    if use_pickle:\n",
    "        f=open('mlp_inning_2.pkl', 'wb')\n",
    "        pickle.dump(mlp_inning,f)\n",
    "        f.close()\n",
    "else:\n",
    "    f=open('mlp_inning_2.pkl', 'rb')\n",
    "    mlp = pickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "\n",
    "print(np.sum(mlp_inning.predict(in_test) == out_test) / len(out_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Umpire\n",
    "\n",
    "It should not be surprising that the largest influence on the accuracy of balls and strikes is the umpire themselves. The list of umpires, accuracy, and number of pitches called shows more variation in accuracy than any of the models to this point have.\n",
    "\n",
    "However, in spite of this, adding umpire as an explanatory variable to a neural network failed to significantly improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mike Winters: 0.9059033989266547, 16770\n",
      "Larry Vanover: 0.9085872576177285, 17328\n",
      "Jeff Nelson: 0.9150052465897167, 18107\n",
      "Dana DeMuth: 0.9104543449492721, 11335\n",
      "Gerry Davis: 0.9078847390262434, 17109\n",
      "Jerry Layne: 0.9070496827999145, 14029\n",
      "Ted Barrett: 0.9082100968893422, 17649\n",
      "Dale Scott: 0.9006440839694656, 8384\n",
      "Joe West: 0.9100594776312387, 19335\n",
      "Tim Welke: 0.8966012446146482, 4178\n",
      "John Hirschbeck: 0.9031155015197568, 7896\n",
      "Brian Gorman: 0.9150270439758564, 12757\n",
      "Gary Cederstrom: 0.9178389621763643, 18269\n",
      "Bill Miller: 0.9128227523336426, 18319\n",
      "Fieldin Culbreth: 0.9151196873473376, 16376\n",
      "Ron Kulpa: 0.913002042205582, 14690\n",
      "Laz Diaz: 0.913472191446931, 18052\n",
      "Ed Hickox: 0.9134708466751061, 13429\n",
      "Dan Iassogna: 0.9073003639477628, 14013\n",
      "Mark Carlson: 0.9153011639733303, 17698\n",
      "Eric Cooper: 0.9199949840115368, 15949\n",
      "Doug Eddings: 0.9082596402574746, 16623\n",
      "Brian Knight: 0.9151686527974147, 14853\n",
      "Chris Guccione: 0.9172604908946952, 17682\n",
      "Paul Nauert: 0.9077962388168705, 16431\n",
      "Phil Cuzzi: 0.9165932821738584, 15898\n",
      "Hunter Wendelstedt: 0.9076191028840016, 17233\n",
      "Angel Hernandez: 0.9055020555273976, 17757\n",
      "CB Bucknor: 0.908795203775751, 15679\n",
      "Kerwin Danley: 0.9087062044931234, 16069\n",
      "Mike Everitt: 0.9183402141207753, 14291\n",
      "Sam Holbrook: 0.9158083296014331, 17864\n",
      "Mike DiMuro: 0.9137186223000584, 8565\n",
      "Rob Drake: 0.9063479090042229, 14682\n",
      "Mark Wegner: 0.9220693812681536, 18247\n",
      "Jim Wolf: 0.913602202971717, 17431\n",
      "Jim Reynolds: 0.9160557389893111, 17869\n",
      "Tony Randazzo: 0.91796243313845, 16078\n",
      "Bob Davidson: 0.9037219678290478, 8517\n",
      "Scott Barry: 0.9155950752393981, 14620\n",
      "Lance Barksdale: 0.9166953331040019, 17442\n",
      "Tim Timmons: 0.9125083166999335, 18036\n",
      "Bill Welke: 0.9177347937543696, 17164\n",
      "Tripp Gibson III: 0.916030534351145, 17161\n",
      "Adam Hamari: 0.9193751024198394, 18307\n",
      "Jerry Meals: 0.913945488224398, 18895\n",
      "Marty Foster: 0.9125499001996008, 16032\n",
      "Quinn Wolcott: 0.9180682364247957, 16648\n",
      "Lance Barrett: 0.9070189828919616, 17068\n",
      "Cory Blaser: 0.9184724051239076, 16706\n",
      "Chris Conroy: 0.9180346492473729, 17605\n",
      "James Hoye: 0.9230536567660759, 16531\n",
      "Paul Schrieber: 0.9179678396644139, 4291\n",
      "Mike Estabrook: 0.911026921531485, 17421\n",
      "Vic Carapazza: 0.9197687861271676, 17300\n",
      "D.J. Reyburn: 0.916671591513504, 16921\n",
      "Will Little: 0.9156357607448029, 15682\n",
      "Todd Tichenor: 0.9164848127396048, 16955\n",
      "David Rackley: 0.9148999346754558, 16839\n",
      "Mike Muchlinski: 0.9182050216655785, 16847\n",
      "Marvin Hudson: 0.9164376030786147, 18190\n",
      "Toby Basner: 0.9213451125676831, 7018\n",
      "Jeff Kellogg: 0.9159395754655554, 15358\n",
      "Tom Hallion: 0.9067053666627177, 16882\n",
      "Jim Joyce: 0.9238608239918474, 6869\n",
      "Manny Gonzalez: 0.9159113796576033, 17874\n",
      "Paul Emmel: 0.9115834873690696, 12984\n",
      "Brian O'Nora: 0.9088264462809917, 15125\n",
      "Alfonso Marquez: 0.9215529088410445, 17464\n",
      "Greg Gibson: 0.921655373242986, 17857\n",
      "Adrian Johnson: 0.9105918647166362, 17504\n",
      "Andy Fletcher: 0.9079796264855687, 17670\n",
      "Alan Porter: 0.9245128923112822, 17142\n",
      "Sean Barber: 0.9241715213446662, 10471\n",
      "Dan Bellino: 0.9169762368064155, 16959\n",
      "Chris Segal: 0.9188824114886961, 14066\n",
      "Jordan Baker: 0.9122277689396018, 18229\n",
      "Mark Ripperger: 0.9197527229908743, 16985\n",
      "Clint Fagan: 0.9158886894075404, 11140\n",
      "Chad Fairchild: 0.9208761734465802, 17896\n",
      "Bruce Dreckman: 0.9141610472034907, 12605\n",
      "Gabe Morales: 0.9232872218273279, 18289\n",
      "John Tumpane: 0.9232512036250354, 17655\n",
      "Ryan Blakney: 0.9145387623648493, 17388\n",
      "Tom Woodring: 0.92219703215169, 9704\n",
      "Pat Hoberg: 0.9272336332258247, 17673\n",
      "Ben May: 0.9221403644823575, 12895\n",
      "Marcus Pattillo: 0.8854568854568855, 777\n",
      "Carlos Torres: 0.9153572892471649, 14638\n",
      "Stu Scheurwater: 0.9222981116914424, 12445\n",
      "Anthony Johnson: 0.9235668789808917, 157\n",
      "Chad Whitson: 0.9158853705996816, 11306\n",
      "Nic Lentz: 0.9206667695632041, 12958\n",
      "Ramon De Jesus: 0.9084908826490659, 8939\n",
      "Roberto Ortiz: 0.9142025089605734, 4464\n",
      "John Libka: 0.9225152129817444, 2465\n",
      "Ryan Additon: 0.924077083817042, 4307\n",
      "Shane Livensparger: 0.9179616913624864, 2767\n",
      "Nick Mahrley: 0.9207732099101552, 3673\n",
      "Jeremie Rehak: 0.9201298701298701, 3080\n",
      "Jansen Visconti: 0.930051182061906, 4103\n"
     ]
    }
   ],
   "source": [
    "umpires = p_ab.umpire_HP.unique()\n",
    "\n",
    "umpire_accuracy = []\n",
    "umpire_count = []\n",
    "for ump in umpires:\n",
    "    ump_df =  p_ab.loc[p_ab.umpire_HP == ump]\n",
    "    ump_acc = np.sum(ump_df['correct']) / len(ump_df)\n",
    "    ump_len = len(ump_df)\n",
    "    print(ump + \": \" + str(ump_acc) + \", \" + str(ump_len))\n",
    "    umpire_accuracy.append(ump_acc)\n",
    "    umpire_count.append(ump_len)\n",
    "  \n",
    "ump_df = p_ab.loc[p_ab.umpire_HP == umpires[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training on just Joe West's data\n",
    "joe_west_df =  p_ab.loc[p_ab.umpire_HP == \"Joe West\"]\n",
    "if not use_pickle or not os.path.isfile('mlp_joe_west.pkl'):\n",
    "    mean_x = np.mean(joe_west_df['px'])\n",
    "    mean_z = np.mean(joe_west_df['pz'])\n",
    "    #Interestingly, although in the end relatively unimportant, the standard deviation of this data ends up being around 1\n",
    "    #Therefore the z scores used here and the distances in the regression example are extremly similar\n",
    "    joe_west_df['z_px'] = (joe_west_df['px'] - mean_x)/joe_west_df.px.std(ddof=0) \n",
    "    joe_west_df['z_pz'] = (joe_west_df['pz'] - mean_z)/joe_west_df.pz.std(ddof=0)\n",
    "\n",
    "    #We split our data into training and test sets again (again)\n",
    "    in_full = joe_west_df[['z_px', 'z_pz']].values\n",
    "    out_full = joe_west_df['call']\n",
    "\n",
    "    in_train, in_test, out_train, out_test = train_test_split(in_full, out_full, test_size = 0.20)\n",
    "\n",
    "    #adam is reccomended for datasets with samples over 1000s\n",
    "    #early stopping will set aside a validation set and stop when that is no longer improving\n",
    "    mlp_joe_west = MLPClassifier(solver='adam', activation='relu', hidden_layer_sizes=(64, 64), random_state=1, early_stopping = True)\n",
    "\n",
    "    mlp_joe_west.fit(in_train, out_train)\n",
    "    if use_pickle:\n",
    "        f=open('mlp_joe_west.pkl', 'wb')\n",
    "        pickle.dump(mlp_joe_west,f)\n",
    "        f.close()\n",
    "else:\n",
    "    f=open('mlp_joe_west.pkl', 'rb')\n",
    "    mlp_joe_west = pickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "\n",
    "print(np.sum(mlp_joe_west.predict(in_test) == out_test) / len(out_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training on all umpire data\n",
    "\n",
    "#One-hot encoding the umpires\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(categorical_features = [0])\n",
    "ump_encoded = pd.get_dummies(p_ab['umpire_HP'])\n",
    "\n",
    "in_df = p_ab[['px', 'pz']].join(ump_encoded)\n",
    "out_df = p_ab[['call']]\n",
    "\n",
    "if not use_pickle or not os.path.isfile('mlp_umpire.pkl'):\n",
    "    mean_x = np.mean(in_df['px'])\n",
    "    mean_z = np.mean(in_df['pz'])\n",
    "    #Interestingly, although in the end relatively unimportant, the standard deviation of this data ends up being around 1\n",
    "    #Therefore the z scores used here and the distances in the regression example are extremly similar\n",
    "    in_df['z_px'] = (in_df['px'] - mean_x)/in_df.px.std(ddof=0) \n",
    "    in_df['z_pz'] = (in_df['pz'] - mean_z)/in_df.pz.std(ddof=0)\n",
    "\n",
    "    #We split our data into training and test sets again (again)\n",
    "    in_full = in_df[['z_px', 'z_pz']].join(ump_encoded).values\n",
    "    out_full = out_df['call']\n",
    "\n",
    "    in_train, in_test, out_train, out_test = train_test_split(in_full, out_full, test_size = 0.20)\n",
    "\n",
    "    #adam is reccomended for datasets with samples over 1000s\n",
    "    #early stopping will set aside a validation set and stop when that is no longer improving\n",
    "    mlp_ump = MLPClassifier(solver='adam', activation='relu', hidden_layer_sizes=(64, 64), random_state=1, early_stopping = True)\n",
    "\n",
    "    mlp_ump.fit(in_train, out_train)\n",
    "    if use_pickle:\n",
    "        f=open('mlp_umpire.pkl', 'wb')\n",
    "        pickle.dump(mlp_ump,f)\n",
    "        f.close()\n",
    "else:\n",
    "    f=open('mlp_umpire.pkl', 'rb')\n",
    "    mlp_ump = pickle.load(f, encoding='latin1')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9165336362869216"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_ump\n",
    "\n",
    "getAccuracy(mlp_ump.predict(in_full), out_full)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
