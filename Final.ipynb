{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Parameters Influencing Human Strikezones\n",
    "\n",
    "## Goals\n",
    "\n",
    "The objective of this project is to determine what factors other than position influence an umpire's accuracy when calling balls and strikes. I plan to look at factors such as inning and batter's handedness that should not influence the data but possibly does as well as factors such as batter's height that should influence ball/strike calls.\n",
    "\n",
    "## Methods\n",
    "\n",
    "I have created multiple models based on a pitch's x and y coordinates as they cross the plate in order to set up a baseline for accuracy. I then will predict data using this model and find the subsets that are more or less accurate. Finally, I will add the factors that most directly influence the data as explanatory variables for a final model.\n",
    "\n",
    "## The data\n",
    "\n",
    "The data is pitch data that comes straight from MLB and has been collected into nice csv files on kaggle (https://www.kaggle.com/pschale/mlb-pitch-data-20152018). Pitches included are from the 2015-2018 seasons. Additionally this dataset includes data that can be joined with the pitch data to get information about the atbat (ie inning, batter, pitcher) and game (ie umpire, weather). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tenor\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "c:\\users\\tenor\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    }
   ],
   "source": [
    "#Data importing and manipulating, mostly from step 1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os.path\n",
    "games = pd.read_csv(\"games.csv\")\n",
    "atbats= pd.read_csv(\"atbats.csv\")\n",
    "players = pd.read_csv(\"player_names.csv\")\n",
    "pitches_full = pd.read_csv(\"pitches2.csv\")\n",
    "pitches = pitches_full\n",
    "pitch = pitches[['px', 'pz', 'code']][pitches['code'].isin([\"C\", \"B\", \"*B\"])] #only balls and called strikes\n",
    "\n",
    "\n",
    "#These two rows are most important for step 2, as they clean the data and transform one column.\n",
    "pitch = pitches[['px', 'pz', 'code', 'ab_id']][pitches['code'].isin([\"C\", \"B\"])].dropna(axis='rows', how='any') #drop NA's\n",
    "pitch['call'] = pitches['code'].isin([\"C\"]) #Call will be True for strikes, False for balls\n",
    "\n",
    "\n",
    "strikes = pitch[['px', 'pz']][pitches['code'].isin([\"C\"])] #We use these later for graphical summaries\n",
    "balls = pitch[['px', 'pz']][pitches['code'].isin([\"B\"])]\n",
    "\n",
    "#Pickle is a great package that stores python variables as files. This is really useful as it saves me from running\n",
    "#every model every time I want to run this document. It also allows me to share the models online so others can view the results immediately.\n",
    "use_pickle = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Models\n",
    "\n",
    "Using only the positional data as the ball crosses the plate, I created four models to predict whether the pitch would be called a ball or a strike. The first, a logistic regression, had an underwhelming performance. The second, a nearest neighbors model, performed quite well. This performance was matched by both of the nerual networks I trained. \n",
    "\n",
    "After evaluating these four models, I took the confidences of one neural network and the nearest neighbors model and averaged them to create a final model, although this step similarly did not improve accuracy.\n",
    "\n",
    "Given that many of these models have topped out at around a 91.5% accuracy, I infer that given these explanatory paramters, this level of accuracy is near the limit we can get due to variance in umpire calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "### Process:\n",
    "I began with a simple logistic regression. It was a simple, although relatively unsuccessful starting point. I first tried giving it px and pz values, but the model just predicted everything to be a ball. However, when giving it distance from the center of the observed values it was able to find success.\n",
    "\n",
    "### Results:\n",
    "In the end I was able to create a moderately successful classifier with 88.7% accuracy.\n",
    "\n",
    "### Remaining Questions:\n",
    "There is no guarentee the mean of the px and pz values is the best for this regression. It would be possible to try measuring distance from a variety of points and seeing which would perform the best, but I belive other models will outperform even the most optimized logistic regression for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tenor\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6659111435017453\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.linear_model as lm\n",
    "#My first attempt at a regression, which found no success\n",
    "in_full = pitch[['px', 'pz']].values\n",
    "out_full = pitch['call']\n",
    "\n",
    "in_train, in_test, out_train, out_test = train_test_split(in_full, out_full, test_size = 0.20)\n",
    "\n",
    "#Now we train the model\n",
    "model = lm.LogisticRegression()\n",
    "model.fit(in_train, out_train)\n",
    "\n",
    "score = model.score(in_test, out_test)\n",
    "print(\"Score: \" + str(score))\n",
    "#That's just the proportion of balls. Not good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.8863611795925057\n",
      "88.7% accuracy is not terrible!\n"
     ]
    }
   ],
   "source": [
    "#This regression found decent success!\n",
    "\n",
    "#I use distance from the center of our observations as our input metric.\n",
    "#Distances have had greater success than pure px and pz values\n",
    "#And by measuring distance from the mean we should get distance from roughly the center of the strike zone. \n",
    "mean_x = np.mean(pitch['px'])\n",
    "mean_z = np.mean(pitch['pz'])\n",
    "pitch['x_dist'] = pitch['px'] - mean_x\n",
    "pitch['z_dist'] = pitch['pz'] - mean_z\n",
    "pitch['distance'] = np.sqrt(pitch['x_dist'] * pitch['x_dist'] + pitch['z_dist'] * pitch['z_dist'])\n",
    "#We split our data into training and test sets\n",
    "\n",
    "in_full = pitch[['distance']].values\n",
    "out_full = pitch['call']\n",
    "\n",
    "in_train, in_test, out_train, out_test = train_test_split(in_full, out_full, test_size = 0.20)\n",
    "\n",
    "\n",
    "if not use_pickle or not os.path.isfile('lm.pkl'):\n",
    "    #Now we train the model\n",
    "    linear_model = lm.LogisticRegression()\n",
    "    linear_model.fit(in_train, out_train)\n",
    "    if use_pickle:\n",
    "        f=open('lm.pkl', 'wb')\n",
    "        pickle.dump(linear_model, f)\n",
    "        f.close()\n",
    "else:\n",
    "    f=open('lm.pkl', 'rb')\n",
    "    linear_model = pickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "    \n",
    "score = linear_model.score(in_test, out_test)\n",
    "print(\"Score: \" + str(score))\n",
    "print(\"88.7% accuracy is not terrible!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbors\n",
    "\n",
    "### Process:\n",
    "As an attempt to create a better model than the logistic regression, I decided to use a K nearest neighbors model. Immediately I was getting success above 90%, but to maximize the model's accuracy I looped through different values for n_neighbors to find the optimal one, which ended up being around 95. I also tried weighting based on distance, which had minimal impact on the model. \n",
    "\n",
    "### Results:\n",
    "The end models were extremely successful with 91.5% for the unweighted model and 91.2% for the distance weighted model.\n",
    "This is quite good!\n",
    "\n",
    "### Remaining Questions:\n",
    "I believe if I spent as much time tweaking the distance weighted model as I did the non-weighted model it could perform as well or better. I also would be interested if there are any meaningful differences in the confidences of these two models\n",
    "The first model is a simple fraction over 95, but I would believe this one could be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here's where we will do the much more successful K-nearest neighbors model\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We split our data into training and test sets again\n",
    "in_full = pitch[['px', 'pz']].values\n",
    "out_full = pitch['call']\n",
    "\n",
    "in_train, in_test, out_train, out_test = train_test_split(in_full, out_full, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I reccomend not running the following code block due to runtime. It creates 50  models with varying n_neigbors values to find the optimal one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lest's find the optimal number of neighbors\n",
    "#In the end I had the best results with n=95\n",
    "#This code block takes a long time to run, I would reccomend ignoring it and just running the next one with the actual model.\n",
    "if(False):\n",
    "    knn_list_50s = []\n",
    "    def KNN(n):\n",
    "        classifier = KNeighborsClassifier(n_neighbors = n)\n",
    "        classifier.fit(in_train, out_train)\n",
    "\n",
    "        #Predicting, this takes a while\n",
    "        test_predictions = classifier.predict(in_test)\n",
    "        return np.sum(test_predictions == out_test) / len(test_predictions)\n",
    "\n",
    "    for i in range(1, 50):\n",
    "        knn_list_50s.append(KNN(i * 5))\n",
    "    knn_list_50s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9145154597474693\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 95)\n",
    "knn.fit(in_train, out_train)\n",
    "\n",
    "#Predicting, this takes a while\n",
    "test_predictions = knn.predict(in_test)\n",
    "    \n",
    "print(str(np.sum(test_predictions == out_test) / len(test_predictions)))\n",
    "#91.5% Accuracy, pretty good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9122035588628098\n"
     ]
    }
   ],
   "source": [
    "knn_distance = KNeighborsClassifier(n_neighbors = 95, weights=\"distance\")\n",
    "knn_distance.fit(in_train, out_train)\n",
    "\n",
    "#Predicting, this takes a while\n",
    "test_predictions = knn_distance.predict(in_test)\n",
    "#Predicting probabilities, I should do this at the same time as above, but I don't because I'm dumb.\n",
    "probabilities = knn_distance.predict_proba(in_test)\n",
    "print(str(np.sum(test_predictions == out_test) / len(test_predictions)))\n",
    "#91.2% Accuracy \n",
    "#Not bad. I believe if I spent as much time tweaking this as I did the non-weighted model it could perform as well or better.\n",
    "#I also would be interested if there are any meaningful differences in the confidences of these two models\n",
    "#The first model is a simple fraction over 95, but I would believe this one could be different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "### Process:\n",
    "For my neural nets I used relu as the activation function. The documentation for this package reccomended using the 'adam' solver for larger datasets, so I utilized that. I trained a net with two hidden layers with 64 neurons each as well as a larger one with three hidden layers of size 128.\n",
    "\n",
    "### Results:\n",
    "Both of these models peformed similary to eachother and to the KNN model. This leads me to believe that, given this data set and these parameters, the best we may be able to do is around 91.5%.\n",
    "\n",
    "As one would expect, these take a while to train, so use of the pickle package is highly reccomended here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6054390693814358\n"
     ]
    }
   ],
   "source": [
    "#Neural Network time!\n",
    "import os.path\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "if not use_pickle or not os.path.isfile('mlp_small.pkl'):\n",
    "    mean_x = np.mean(pitch['px'])\n",
    "    mean_z = np.mean(pitch['pz'])\n",
    "    #Interestingly, although in the end relatively unimportant, the standard deviation of this data ends up being around 1\n",
    "    #Therefore the z scores used here and the distances in the regression example are extremly similar\n",
    "    pitch['z_px'] = (pitch['px'] - mean_x)/pitch.px.std(ddof=0) \n",
    "    pitch['z_pz'] = (pitch['pz'] - mean_z)/pitch.pz.std(ddof=0)\n",
    "\n",
    "    #We split our data into training and test sets again (again)\n",
    "    in_full = pitch[['z_px', 'z_pz']].values\n",
    "    out_full = pitch['call']\n",
    "\n",
    "    in_train, in_test, out_train, out_test = train_test_split(in_full, out_full, test_size = 0.20)\n",
    "\n",
    "    #adam is reccomended for datasets with samples over 1000s\n",
    "    #early stopping will set aside a validation set and stop when that is no longer improving\n",
    "    mlp = MLPClassifier(solver='adam', activation='relu', hidden_layer_sizes=(64, 64), random_state=1, early_stopping = True)\n",
    "\n",
    "    mlp.fit(in_train, out_train)\n",
    "    if use_pickle:\n",
    "        f=open('mlp_small.pkl', 'wb')\n",
    "        pickle.dump(mlp,f)\n",
    "        f.close()\n",
    "else:\n",
    "    f=open('mlp_small.pkl', 'rb')\n",
    "    mlp = pickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "\n",
    "print(np.sum(mlp.predict(in_test) == out_test) / len(out_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6080438807854186\n"
     ]
    }
   ],
   "source": [
    "#Neural net 2, more hidden layers this time because why not?\n",
    "if not use_pickle or not os.path.isfile('mlp_large.pkl'):\n",
    "    mean_x = np.mean(pitch['px'])\n",
    "    mean_z = np.mean(pitch['pz'])\n",
    "    #Interestingly, although in the end relatively unimportant, the standard deviation of this data ends up being around 1\n",
    "    #Therefore the z scores used here and the distances in the regression example are extremly similar\n",
    "    pitch['z_px'] = (pitch['px'] - mean_x)/pitch.px.std(ddof=0) \n",
    "    pitch['z_pz'] = (pitch['pz'] - mean_z)/pitch.pz.std(ddof=0)\n",
    "\n",
    "    #We split our data into training and test sets again (again)\n",
    "    in_full = pitch[['z_px', 'z_pz']].values\n",
    "    out_full = pitch['call']\n",
    "\n",
    "    in_train, in_test, out_train, out_test = train_test_split(in_full, out_full, test_size = 0.20)\n",
    "\n",
    "    #adam is reccomended for datasets with samples over 1000s\n",
    "    #early stopping will set aside a validation set and stop when that is no longer improving\n",
    "    mlp_large = MLPClassifier(solver='adam', activation='relu', hidden_layer_sizes=(128, 128, 128), random_state=1, early_stopping = True)\n",
    "\n",
    "    mlp_large.fit(in_train, out_train)\n",
    "    if use_pickle:\n",
    "        f=open('mlp_large.pkl', 'wb')\n",
    "        pickle.dump(mlp_large, f)\n",
    "        f.close()\n",
    "else:\n",
    "    f=open('mlp_large.pkl', 'rb')\n",
    "    mlp_large = pickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "\n",
    "print(np.sum(mlp_large.predict(in_test) == out_test) / len(out_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_x = np.mean(pitch['px'])\n",
    "mean_z = np.mean(pitch['pz'])\n",
    "#Interestingly, although in the end relatively unimportant, the standard deviation of this data ends up being around 1\n",
    "#Therefore the z scores used here and the distances in the regression example are extremly similar\n",
    "pitch['z_px'] = (pitch['px'] - mean_x)/pitch.px.std(ddof=0) \n",
    "pitch['z_pz'] = (pitch['pz'] - mean_z)/pitch.pz.std(ddof=0)\n",
    "\n",
    "#The models probabilites are returned as a 2d list where each element has the probability of it being \n",
    "    \n",
    "\n",
    "knn_prob = knn.predict_proba(pitch[['px', 'pz']])\n",
    "knn_ps = []\n",
    "for i in knn_prob:\n",
    "    knn_ps.append(i[1])\n",
    "    \n",
    "pitch['knn_prob'] = knn_ps\n",
    "\n",
    "\n",
    "nn_prob = mlp.predict_proba(pitch[['z_px', 'z_pz']])\n",
    "nn_ps = []\n",
    "for i in nn_prob:\n",
    "    nn_ps.append(i[1])\n",
    "\n",
    "pitch['nn_prob'] = nn_ps\n",
    "\n",
    "\n",
    "pitch['prob'] = (pitch['knn_prob'] + pitch['nn_prob']) / 2\n",
    "pitch['predict'] = pitch['prob'] > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9149261132184425% accuracy for the combined model\n",
      "We don't really gain any accuracy combining these models, but I'm going to be using it from here on out.\n"
     ]
    }
   ],
   "source": [
    "print(str(np.sum(pitch['predict'] == pitch['call']) / len(pitch)) + \"% accuracy for the combined model\")\n",
    "print(\"We don't really gain any accuracy combining these models, but I'm going to be using it from here on out.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Location parameters\n",
    "\n",
    "### Batter Handedness\n",
    "\n",
    "I predicted batter handedness to be one of the most influential factors. However, to my surprise, the accuracy of our model was not significantly different for lefties or righties and adding handedness to the nearest neighbors model only improved it marginally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on left handed batters: 0.9146963997850618%\n",
      "Accuracy on right handed batters: 0.915096390297216%\n"
     ]
    }
   ],
   "source": [
    "ab = atbats[['ab_id', 'batter_id', 'p_throws', 'stand', 'pitcher_id', 'top', 'inning']]\n",
    "\n",
    "#This is where it would have been nice to import the data into a SQL database\n",
    "#But the benefit of not doing that is that this is much easier to share. My last project\n",
    "#Was difficult to put online due to the database queries\n",
    "\n",
    "#Joining the atbats and pitches on ab_id\n",
    "p_ab = pd.merge(ab, pitch, on='ab_id')\n",
    "\n",
    "#Splitting on left and right\n",
    "left = p_ab.loc[p_ab['stand'] == \"L\"]\n",
    "right = p_ab.loc[p_ab['stand'] == \"R\"]\n",
    "\n",
    "def getAccuracy(predictions, actual):\n",
    "    return (np.sum(predictions == actual) / (len(actual)))\n",
    "\n",
    "print(\"Accuracy on left handed batters: \" + str(getAccuracy(left['predict'], left['call'])) + \"%\")\n",
    "print(\"Accuracy on right handed batters: \" + str(getAccuracy(right['predict'], right['call'])) + \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9158440181744003\n",
      "Considering our 91.45% accuracy for our base k-nearest neigbors model, a 91.73% accuracy isn't a big improvement\n"
     ]
    }
   ],
   "source": [
    "stand_float = []\n",
    "for i in p_ab['stand']:\n",
    "    if i == \"L\":\n",
    "        stand_float.append(5)\n",
    "    else:\n",
    "        stand_float.append(0)\n",
    "p_ab['stand_float'] = stand_float\n",
    "in_full = p_ab[['px', 'pz', 'stand_float']].values\n",
    "out_full = p_ab['call']\n",
    "\n",
    "in_train, in_test, out_train, out_test = train_test_split(in_full, out_full, test_size = 0.20)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 95)\n",
    "knn.fit(in_train, out_train)\n",
    "\n",
    "#Predicting, this takes a while\n",
    "test_predictions = knn.predict(in_test)\n",
    "    \n",
    "print(str(np.sum(test_predictions == out_test) / len(test_predictions)))\n",
    "print(\"Considering our 91.45% accuracy for our base k-nearest neigbors model, a 91.73% accuracy isn't a big improvement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Home/Away\n",
    "\n",
    "While I don't expect there to be a significant difference in calls for the home or away team, finding a difference in this area would be problematic, so I decided to check. Fortuantely, based both on the lack of difference in the model's accuracy on home team at bats vs away team at bats as well as the model's lack of improvement when considering the half of the inning indicates that this is not something to worry about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model accuracy on away team at bats: 0.9145569489976411%\n",
      "Original model accuracy on home team at bats: 0.9153785283172989%\n"
     ]
    }
   ],
   "source": [
    "away_atbats = p_ab.loc[p_ab[\"top\"] == True]\n",
    "home_atbats = p_ab.loc[p_ab[\"top\"] == False]\n",
    "\n",
    "print(\"Original model accuracy on away team at bats: \" + str(getAccuracy(away_atbats['predict'], away_atbats['call'])) + \"%\")\n",
    "print(\"Original model accuracy on home team at bats: \" + str(getAccuracy(home_atbats['predict'], home_atbats['call'])) + \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.914470128357574\n"
     ]
    }
   ],
   "source": [
    "top_float = []\n",
    "for i in p_ab['top']:\n",
    "    if i:\n",
    "        top_float.append(1)\n",
    "    else:\n",
    "        top_float.append(0)\n",
    "p_ab['top_float'] = top_float\n",
    "in_full = p_ab[['px', 'pz', 'top_float']].values\n",
    "out_full = p_ab['call']\n",
    "\n",
    "in_train, in_test, out_train, out_test = train_test_split(in_full, out_full, test_size = 0.20)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 95)\n",
    "knn.fit(in_train, out_train)\n",
    "\n",
    "#Predicting, this takes a while\n",
    "test_predictions = knn.predict(in_test)\n",
    "print(str(np.sum(test_predictions == out_test) / len(test_predictions)))\n",
    "print(\"The 0.01% improvement isn't exactyly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inning Number\n",
    "\n",
    "atbats.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9140168144586212\n"
     ]
    }
   ],
   "source": [
    "\n",
    "in_full = p_ab[['px', 'pz', 'inning']].values\n",
    "out_full = p_ab['call']\n",
    "\n",
    "in_train, in_test, out_train, out_test = train_test_split(in_full, out_full, test_size = 0.20)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 95)\n",
    "knn.fit(in_train, out_train)\n",
    "\n",
    "#Predicting, this takes a while\n",
    "test_predictions = knn.predict(in_test)\n",
    "print(str(np.sum(test_predictions == out_test) / len(test_predictions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mlp_inning' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-d60429bb9a69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmlp_inning\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mout_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'mlp_inning' is not defined"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "if not use_pickle or not os.path.isfile('mlp_inning_2.pkl'):\n",
    "    mean_x = np.mean(p_ab['px'])\n",
    "    mean_z = np.mean(p_ab['pz'])\n",
    "    #Interestingly, although in the end relatively unimportant, the standard deviation of this data ends up being around 1\n",
    "    #Therefore the z scores used here and the distances in the regression example are extremly similar\n",
    "    p_ab['z_px'] = (p_ab['px'] - mean_x)/p_ab.px.std(ddof=0) \n",
    "    p_ab['z_pz'] = (p_ab['pz'] - mean_z)/p_ab.pz.std(ddof=0)\n",
    "\n",
    "    #We split our data into training and test sets again (again)\n",
    "    in_full = p_ab[['z_px', 'z_pz', 'inning']].values\n",
    "    out_full = p_ab['call']\n",
    "\n",
    "    in_train, in_test, out_train, out_test = train_test_split(in_full, out_full, test_size = 0.20)\n",
    "\n",
    "    #adam is reccomended for datasets with samples over 1000s\n",
    "    #early stopping will set aside a validation set and stop when that is no longer improving\n",
    "    mlp_inning = MLPClassifier(solver='adam', activation='relu', hidden_layer_sizes=(64, 64), random_state=1, early_stopping = True)\n",
    "\n",
    "    mlp_inning.fit(in_train, out_train)\n",
    "    if use_pickle:\n",
    "        f=open('mlp_inning_2.pkl', 'wb')\n",
    "        pickle.dump(mlp_inning,f)\n",
    "        f.close()\n",
    "else:\n",
    "    f=open('mlp_inning_2.pkl', 'rb')\n",
    "    mlp = pickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "\n",
    "print(np.sum(mlp_inning.predict(in_test) == out_test) / len(out_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Umpire\n",
    "\n",
    "It should not be surprising that the largest influence on the accuracy of balls and strikes is the umpire themselves. The umpires that have the highest accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining pitch/atbat dataframe with the game csv on g_id (game id)\n",
    "p_ab = pd.merge(p_ab, atbats[['ab_id', 'g_id']], on='ab_id')\n",
    "p_ab = pd.merge(games[['umpire_HP', 'g_id']], p_ab, on='g_id')\n",
    "\n",
    "\n",
    "p_ab['correct'] = p_ab['predict'] == p_ab['call']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Mike Winters', 'Larry Vanover', 'Jeff Nelson', 'Dana DeMuth',\n",
       "       'Gerry Davis', 'Jerry Layne', 'Ted Barrett', 'Dale Scott',\n",
       "       'Joe West', 'Tim Welke', 'John Hirschbeck', 'Brian Gorman',\n",
       "       'Gary Cederstrom', 'Bill Miller', 'Fieldin Culbreth', 'Ron Kulpa',\n",
       "       'Laz Diaz', 'Ed Hickox', 'Dan Iassogna', 'Mark Carlson',\n",
       "       'Eric Cooper', 'Doug Eddings', 'Brian Knight', 'Chris Guccione',\n",
       "       'Paul Nauert', 'Phil Cuzzi', 'Hunter Wendelstedt',\n",
       "       'Angel Hernandez', 'CB Bucknor', 'Kerwin Danley', 'Mike Everitt',\n",
       "       'Sam Holbrook', 'Mike DiMuro', 'Rob Drake', 'Mark Wegner',\n",
       "       'Jim Wolf', 'Jim Reynolds', 'Tony Randazzo', 'Bob Davidson',\n",
       "       'Scott Barry', 'Lance Barksdale', 'Tim Timmons', 'Bill Welke',\n",
       "       'Tripp Gibson III', 'Adam Hamari', 'Jerry Meals', 'Marty Foster',\n",
       "       'Quinn Wolcott', 'Lance Barrett', 'Cory Blaser', 'Chris Conroy',\n",
       "       'James Hoye', 'Paul Schrieber', 'Mike Estabrook', 'Vic Carapazza',\n",
       "       'D.J. Reyburn', 'Will Little', 'Todd Tichenor', 'David Rackley',\n",
       "       'Mike Muchlinski', 'Marvin Hudson', 'Toby Basner', 'Jeff Kellogg',\n",
       "       'Tom Hallion', 'Jim Joyce', 'Manny Gonzalez', 'Paul Emmel',\n",
       "       \"Brian O'Nora\", 'Alfonso Marquez', 'Greg Gibson', 'Adrian Johnson',\n",
       "       'Andy Fletcher', 'Alan Porter', 'Sean Barber', 'Dan Bellino',\n",
       "       'Chris Segal', 'Jordan Baker', 'Mark Ripperger', 'Clint Fagan',\n",
       "       'Chad Fairchild', 'Bruce Dreckman', 'Gabe Morales', 'John Tumpane',\n",
       "       'Ryan Blakney', 'Tom Woodring', 'Pat Hoberg', 'Ben May',\n",
       "       'Marcus Pattillo', 'Carlos Torres', 'Stu Scheurwater',\n",
       "       'Anthony Johnson', 'Chad Whitson', 'Nic Lentz', 'Ramon De Jesus',\n",
       "       'Roberto Ortiz', 'John Libka', 'Ryan Additon',\n",
       "       'Shane Livensparger', 'Nick Mahrley', 'Jeremie Rehak',\n",
       "       'Jansen Visconti'], dtype=object)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "umpires = p_ab.umpire_HP.unique()\n",
    "umpires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mike Winters: 0.9064400715563506, 16770\n",
      "Larry Vanover: 0.9091643582640813, 17328\n",
      "Jeff Nelson: 0.9144529739879604, 18107\n",
      "Dana DeMuth: 0.9105425672695192, 11335\n",
      "Gerry Davis: 0.9061312759366416, 17109\n",
      "Jerry Layne: 0.9072635255542092, 14029\n",
      "Ted Barrett: 0.9089466825315882, 17649\n",
      "Dale Scott: 0.9005248091603053, 8384\n",
      "Joe West: 0.9098008792345488, 19335\n",
      "Tim Welke: 0.8973192915270465, 4178\n",
      "John Hirschbeck: 0.9036220871327254, 7896\n",
      "Brian Gorman: 0.9149486556400408, 12757\n",
      "Gary Cederstrom: 0.916853686572883, 18269\n",
      "Bill Miller: 0.9134778099241225, 18319\n",
      "Fieldin Culbreth: 0.9146922325354176, 16376\n",
      "Ron Kulpa: 0.9133424098025867, 14690\n",
      "Laz Diaz: 0.913472191446931, 18052\n",
      "Ed Hickox: 0.9129495867153176, 13429\n",
      "Dan Iassogna: 0.9077285377863412, 14013\n",
      "Mark Carlson: 0.9154706746525031, 17698\n",
      "Eric Cooper: 0.9199949840115368, 15949\n",
      "Doug Eddings: 0.9085604283222042, 16623\n",
      "Brian Knight: 0.9153033057294823, 14853\n",
      "Chris Guccione: 0.9163556158805565, 17682\n",
      "Paul Nauert: 0.9079788205221837, 16431\n",
      "Phil Cuzzi: 0.9163416781985155, 15898\n",
      "Hunter Wendelstedt: 0.9080253002959439, 17233\n",
      "Angel Hernandez: 0.9055583713465112, 17757\n",
      "CB Bucknor: 0.9089227629313094, 15679\n",
      "Kerwin Danley: 0.908643972867011, 16069\n",
      "Mike Everitt: 0.9184801623399342, 14291\n",
      "Sam Holbrook: 0.916703985669503, 17864\n",
      "Mike DiMuro: 0.9138353765323993, 8565\n",
      "Rob Drake: 0.9064841302274894, 14682\n",
      "Mark Wegner: 0.9233298624431413, 18247\n",
      "Jim Wolf: 0.9138316791922437, 17431\n",
      "Jim Reynolds: 0.9159438133079635, 17869\n",
      "Tony Randazzo: 0.9184600074636149, 16078\n",
      "Bob Davidson: 0.9043090290008219, 8517\n",
      "Scott Barry: 0.9156634746922024, 14620\n",
      "Lance Barksdale: 0.9165806673546611, 17442\n",
      "Tim Timmons: 0.9113994233754713, 18036\n",
      "Bill Welke: 0.9177930552318807, 17164\n",
      "Tripp Gibson III: 0.9161470776761261, 17161\n",
      "Adam Hamari: 0.9194843502485388, 18307\n",
      "Jerry Meals: 0.9141571844403281, 18895\n",
      "Marty Foster: 0.9121756487025948, 16032\n",
      "Quinn Wolcott: 0.9167467563671312, 16648\n",
      "Lance Barrett: 0.9065502695101945, 17068\n",
      "Cory Blaser: 0.9179336765234047, 16706\n",
      "Chris Conroy: 0.9185458676512355, 17605\n",
      "James Hoye: 0.9233561188070897, 16531\n",
      "Paul Schrieber: 0.9170356560242368, 4291\n",
      "Mike Estabrook: 0.9110843235175937, 17421\n",
      "Vic Carapazza: 0.9200578034682081, 17300\n",
      "D.J. Reyburn: 0.9160806098930323, 16921\n",
      "Will Little: 0.9151256217319219, 15682\n",
      "Todd Tichenor: 0.9176054261279858, 16955\n",
      "David Rackley: 0.9154344082190153, 16839\n",
      "Mike Muchlinski: 0.9171365821808037, 16847\n",
      "Marvin Hudson: 0.9160527762506872, 18190\n",
      "Toby Basner: 0.9217725847819892, 7018\n",
      "Jeff Kellogg: 0.9164604766245605, 15358\n",
      "Tom Hallion: 0.9074754176045492, 16882\n",
      "Jim Joyce: 0.9244431503857913, 6869\n",
      "Manny Gonzalez: 0.9150162246838984, 17874\n",
      "Paul Emmel: 0.9120455945779421, 12984\n",
      "Brian O'Nora: 0.9086280991735537, 15125\n",
      "Alfonso Marquez: 0.9217246907924874, 17464\n",
      "Greg Gibson: 0.9222153777230218, 17857\n",
      "Adrian Johnson: 0.9103633455210237, 17504\n",
      "Andy Fletcher: 0.9076400679117148, 17670\n",
      "Alan Porter: 0.9245128923112822, 17142\n",
      "Sean Barber: 0.9230254989972304, 10471\n",
      "Dan Bellino: 0.9167403738427974, 16959\n",
      "Chris Segal: 0.9185269444049481, 14066\n",
      "Jordan Baker: 0.9122277689396018, 18229\n",
      "Mark Ripperger: 0.9201059758610539, 16985\n",
      "Clint Fagan: 0.9159784560143627, 11140\n",
      "Chad Fairchild: 0.9208761734465802, 17896\n",
      "Bruce Dreckman: 0.9138437128123761, 12605\n",
      "Gabe Morales: 0.9228498004264859, 18289\n",
      "John Tumpane: 0.9236476918719909, 17655\n",
      "Ryan Blakney: 0.9148263170002301, 17388\n",
      "Tom Woodring: 0.9229183841714756, 9704\n",
      "Pat Hoberg: 0.9266112148475075, 17673\n",
      "Ben May: 0.9223730127956572, 12895\n",
      "Marcus Pattillo: 0.8867438867438867, 777\n",
      "Carlos Torres: 0.9154939199344173, 14638\n",
      "Stu Scheurwater: 0.9218159903575733, 12445\n",
      "Anthony Johnson: 0.9235668789808917, 157\n",
      "Chad Whitson: 0.9162391650451088, 11306\n",
      "Nic Lentz: 0.9204352523537583, 12958\n",
      "Ramon De Jesus: 0.9086027519856807, 8939\n",
      "Roberto Ortiz: 0.9137544802867383, 4464\n",
      "John Libka: 0.9233265720081136, 2465\n",
      "Ryan Additon: 0.923380543301602, 4307\n",
      "Shane Livensparger: 0.9179616913624864, 2767\n",
      "Nick Mahrley: 0.919411924857065, 3673\n",
      "Jeremie Rehak: 0.9204545454545454, 3080\n",
      "Jansen Visconti: 0.929807457957592, 4103\n"
     ]
    }
   ],
   "source": [
    "umpire_accuracy = []\n",
    "umpire_count = []\n",
    "for ump in umpires:\n",
    "    ump_df =  p_ab.loc[p_ab.umpire_HP == ump]\n",
    "    ump_acc = np.sum(ump_df['correct']) / len(ump_df)\n",
    "    ump_len = len(ump_df)\n",
    "    print(ump + \": \" + str(ump_acc) + \", \" + str(ump_len))\n",
    "    umpire_accuracy.append(ump_acc)\n",
    "    umpire_count.append(ump_len)\n",
    "  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "ump_df = p_ab.loc[p_ab.umpire_HP == umpires[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9045771916214119\n"
     ]
    }
   ],
   "source": [
    "#Training on just Joe West's data\n",
    "joe_west_df =  p_ab.loc[p_ab.umpire_HP == \"Joe West\"]\n",
    "if not use_pickle or not os.path.isfile('mlp_joe_west.pkl'):\n",
    "    mean_x = np.mean(joe_west_df['px'])\n",
    "    mean_z = np.mean(joe_west_df['pz'])\n",
    "    #Interestingly, although in the end relatively unimportant, the standard deviation of this data ends up being around 1\n",
    "    #Therefore the z scores used here and the distances in the regression example are extremly similar\n",
    "    joe_west_df['z_px'] = (joe_west_df['px'] - mean_x)/joe_west_df.px.std(ddof=0) \n",
    "    joe_west_df['z_pz'] = (joe_west_df['pz'] - mean_z)/joe_west_df.pz.std(ddof=0)\n",
    "\n",
    "    #We split our data into training and test sets again (again)\n",
    "    in_full = joe_west_df[['z_px', 'z_pz']].values\n",
    "    out_full = joe_west_df['call']\n",
    "\n",
    "    in_train, in_test, out_train, out_test = train_test_split(in_full, out_full, test_size = 0.20)\n",
    "\n",
    "    #adam is reccomended for datasets with samples over 1000s\n",
    "    #early stopping will set aside a validation set and stop when that is no longer improving\n",
    "    mlp_joe_west = MLPClassifier(solver='adam', activation='relu', hidden_layer_sizes=(64, 64), random_state=1, early_stopping = True)\n",
    "\n",
    "    mlp_joe_west.fit(in_train, out_train)\n",
    "    if use_pickle:\n",
    "        f=open('mlp_joe_west.pkl', 'wb')\n",
    "        pickle.dump(mlp_joe_west,f)\n",
    "        f.close()\n",
    "else:\n",
    "    f=open('mlp_joe_west.pkl', 'rb')\n",
    "    mlp = pickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "\n",
    "print(np.sum(mlp_joe_west.predict(in_test) == out_test) / len(out_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
